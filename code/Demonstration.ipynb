{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da5cbc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d99fc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"myapp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3051c280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/apache-spark/3.2.1/libexec\n"
     ]
    }
   ],
   "source": [
    "print(os.environ['SPARK_HOME'])\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f757a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Method to create classification Report\n",
    "def makeClassificationReport(metricsArray):\n",
    "    TN = metricsArray[0][0]\n",
    "    FN = metricsArray[0][1]\n",
    "    FP = metricsArray[1][0]\n",
    "    TP = metricsArray[1][1]\n",
    "    Accuracy = (TP+TN)/(TP+FN+TN+FP)\n",
    "    Precision = TP/(TP+FP)\n",
    "    Recall = TP / (TP + FN)\n",
    "    F1Score = 2*(Precision * Recall)/(Precision + Recall)\n",
    "    print(\"Classification Report\")\n",
    "    print(\"Accuracy: \", Accuracy)\n",
    "    print(\"Precision: \",Precision)\n",
    "    print(\"Recall: \",Recall)\n",
    "    print(\"F1- Score: \", F1Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4395ee1c",
   "metadata": {},
   "source": [
    "## 1. READING DEMONSTRATION DATA \n",
    "- We are reading the cleaned dataset set prepared in previous Juypter Notebook\n",
    "- Data is stored on S3 in following location:  s3://brfss-big-data-project/HeartRiskData/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "605d08cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ LOCAL DATA FILE\n",
    "# Comment if reading from S3\n",
    "\n",
    "heartData = spark.read.csv(\"../../../BRFSS/HeartRiskData/\", header='true',inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7b04faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ FROM S3 BUCKET\n",
    "#Comment if reading locally\n",
    "# sc._jsc.hadoopConfiguration().set(\"fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "# sc._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "# sc._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\",\"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")\n",
    "\n",
    "# heartData = spark.read.csv(\"s3a://brfss-big-data-project/HeartRiskData/\", header = 'true',inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ff9b4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/31 02:29:32 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 4:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset Count: 52052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "numericCols = ['BMI','HighChol','CholCheck','FruitConsume','VegetableConsume','Smoker','HeavyDrinker', \\\n",
    "               'Diabetes','Stroke','Healthcare','NoDoctorDueToCost','PhysicalActivity','GeneralHealth', \\\n",
    "               'PhysicalHealth','MentalHealth','DifficultyWalking','Gender','Age','Education','Income']\n",
    "assembler = VectorAssembler(inputCols=numericCols, outputCol=\"features\")\n",
    "df = assembler.transform(heartData)\n",
    "\n",
    "# We are using the 10% data that was previously reserved (same seed)\n",
    "modelData, demoData = df.randomSplit([0.9, 0.1], seed = 2018)\n",
    "print(\"Test Dataset Count: \" + str(demoData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28fc3a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----+------+--------+---------+------------+----------------+------+------------+--------+------+----------+-----------------+----------------+-------------+--------------+------------+-----------------+------+---+---------+------+--------------------+\n",
      "|HeartDisease|State|  BMI|HighBP|HighChol|CholCheck|FruitConsume|VegetableConsume|Smoker|HeavyDrinker|Diabetes|Stroke|Healthcare|NoDoctorDueToCost|PhysicalActivity|GeneralHealth|PhysicalHealth|MentalHealth|DifficultyWalking|Gender|Age|Education|Income|            features|\n",
      "+------------+-----+-----+------+--------+---------+------------+----------------+------+------------+--------+------+----------+-----------------+----------------+-------------+--------------+------------+-----------------+------+---+---------+------+--------------------+\n",
      "|         0.0| 22.0|18.56|   0.0|     1.0|      1.0|         1.0|             1.0|   1.0|         0.0|     0.0|   0.0|       1.0|              0.0|             1.0|          4.0|           0.0|         0.0|              0.0|   0.0|7.0|      4.0|   8.0|(20,[0,1,2,3,4,5,...|\n",
      "|         0.0| 22.0|20.01|   0.0|     0.0|      1.0|         0.0|             0.0|   0.0|         0.0|     0.0|   0.0|       1.0|              0.0|             1.0|          4.0|          30.0|        10.0|              0.0|   0.0|7.0|      6.0|   8.0|(20,[0,2,9,11,12,...|\n",
      "+------------+-----+-----+------+--------+---------+------------+----------------+------+------------+--------+------+----------+-----------------+----------------+-------------+--------------+------------+-----------------+------+---+---------+------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demoData.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d3fedc",
   "metadata": {},
   "source": [
    "## 3. READING SAVED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2983fe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c7f7de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import PipelineModel\n",
    "mPath =\"../model/\"\n",
    "persistedModel = RandomForestClassificationModel.load(mPath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2591e763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71d252bc",
   "metadata": {},
   "source": [
    "##  4. Predict on the Reserved Demonstration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d01ecc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "|HeartDisease|prediction|\n",
      "+------------+----------+\n",
      "|         0.0|       0.0|\n",
      "|         0.0|       0.0|\n",
      "|         0.0|       1.0|\n",
      "|         0.0|       0.0|\n",
      "|         0.0|       0.0|\n",
      "|         0.0|       0.0|\n",
      "|         0.0|       0.0|\n",
      "|         0.0|       0.0|\n",
      "|         0.0|       0.0|\n",
      "|         0.0|       1.0|\n",
      "+------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "predictionsDF = persistedModel.transform(demoData)\n",
    "predictionsDF.select(\"HeartDisease\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f24b3f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1ceec1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66028137",
   "metadata": {},
   "source": [
    "## 5. FIND OUT A PERSONS HEART RISK\n",
    "- Enter parameters and predict using model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eea141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d60f5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79f6de2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99108168",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
