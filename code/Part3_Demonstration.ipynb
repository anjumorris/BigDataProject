{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5cbc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d99fc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"myapp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3051c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.environ['SPARK_HOME'])\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f757a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Method to create classification Report\n",
    "def makeClassificationReport(metricsArray):\n",
    "    TN = metricsArray[0][0]\n",
    "    FN = metricsArray[1][0]\n",
    "    FP = metricsArray[0][1]\n",
    "    TP = metricsArray[1][1]\n",
    "    Accuracy = (TP+TN)/(TP+FN+TN+FP)\n",
    "    Precision = TP/(TP+FP)\n",
    "    Recall = TP / (TP + FN)\n",
    "    F1Score = 2*(Precision * Recall)/(Precision + Recall)\n",
    "    print(\"Classification Report\")\n",
    "    print(\"Accuracy: \", Accuracy)\n",
    "    print(\"Precision: \",Precision)\n",
    "    print(\"Recall: \",Recall)\n",
    "    print(\"F1- Score: \", F1Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4395ee1c",
   "metadata": {},
   "source": [
    "## 1. READING DEMONSTRATION DATA \n",
    "- We are reading the cleaned dataset set prepared in previous Juypter Notebook\n",
    "- Data is stored on S3 in following location:  s3://brfss-big-data-project/HeartRiskData/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605d08cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ LOCAL DATA FILE\n",
    "# Comment if reading from S3\n",
    "\n",
    "# heartData = spark.read.csv(\"../../../BRFSS/HeartRiskData/\", header='true',inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b04faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ FROM S3 BUCKET\n",
    "#Comment if reading locally\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\",\"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")\n",
    "\n",
    "heartData = spark.read.csv(\"s3a://brfss-big-data-project/HeartRiskData/\", header = 'true',inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff9b4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "numericCols = ['BMI','HighChol','CholCheck','FruitConsume','VegetableConsume','Smoker','HeavyDrinker', \\\n",
    "               'Diabetes','Stroke','Healthcare','NoDoctorDueToCost','PhysicalActivity','GeneralHealth', \\\n",
    "               'PhysicalHealth','MentalHealth','DifficultyWalking','Gender','Age','Education','Income']\n",
    "assembler = VectorAssembler(inputCols=numericCols, outputCol=\"features\")\n",
    "df = assembler.transform(heartData)\n",
    "\n",
    "# We are using the 10% data that was previously reserved (same seed)\n",
    "modelData, demoData = df.randomSplit([0.9, 0.1], seed = 2018)\n",
    "print(\"Demonstration Dataset Count: \" + str(demoData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae570882",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5c804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d3fedc",
   "metadata": {},
   "source": [
    "## 3. READING SAVED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2983fe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099b191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ LOCAL MODEL\n",
    "# Comment if Reading from S3\n",
    "# mPath = \"../model/\"\n",
    "# persistedModel = RandomForestClassificationModel.load(mPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e75a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ MODEL FROM S3\n",
    "# Comment if reading locally stored model\n",
    "mPath = \"s3a://brfss-big-data-project/model/\"\n",
    "persistedModel = RandomForestClassificationModel.load(mPath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d252bc",
   "metadata": {},
   "source": [
    "##  4. MAKING PREDICTIONS ON UNSEEN DEMONSTRATION DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d01ecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "predictions = persistedModel.transform(demoData)\n",
    "predictions.select(\"HeartDisease\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f24b3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"HeartDisease\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %s\" % (accuracy))\n",
    "print(\"Test Error = %s\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7465c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.types import FloatType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "preds_and_labels = predictions.select(['prediction','HeartDisease']).withColumn('HeartDisease', F.col('HeartDisease').cast(FloatType())).orderBy('prediction')\n",
    "preds_and_labels = preds_and_labels.select(['prediction','HeartDisease'])\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "print(\"Confusuion Matrix:\")\n",
    "print(metrics.confusionMatrix().toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2de50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "makeClassificationReport(metrics.confusionMatrix().toArray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1647b7e",
   "metadata": {},
   "source": [
    "## 5. MAKING PREDICTIONS BASED ON AN INDIVIUAL's DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96de2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "numericCols = ['BMI','HighChol','CholCheck','FruitConsume','VegetableConsume','Smoker','HeavyDrinker', \\\n",
    "               'Diabetes','Stroke','Healthcare','NoDoctorDueToCost','PhysicalActivity','GeneralHealth', \\\n",
    "               'PhysicalHealth','MentalHealth','DifficultyWalking','Gender','Age','Education','Income']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb67ea60",
   "metadata": {},
   "source": [
    "#### 5.1 Predicting on person with poor health and habits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9841cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "poorHealthPerson = [{ \"HeartDisease\" : -1, \"State\" : 22 , \"BMI\" : 35, \"HighBP\" : 1, \"HighChol\" : 1, \"CholCheck\" :1, \\\n",
    "             \"FruitConsume\" : 0, \"VegetableConsume\" : 0 , \"Smoker\" : 1 , \"HeavyDrinker\" : 1, \\\n",
    "                    \"Diabetes\" : 1 , \"Stroke\" : 1 , \"Healthcare\": 0 , \"NoDoctorDueToCost\" : 1, \\\n",
    "                    \"PhysicalActivity\" : 0 , \"GeneralHealth\": 1 ,\"PhysicalHealth\" : 20 , \"MentalHealth\" : 5 ,\\\n",
    "                    \"DifficultyWalking\" : 1 , \"Gender\" : 1 ,\"Age\" : 10 , \"Education\" : 2 , \"Income\": 1 }]\n",
    "             \n",
    "dfPoorHealth = spark.createDataFrame(poorHealthPerson)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=numericCols, outputCol=\"features\")\n",
    "poorHealthSample= assembler.transform(dfPoorHealth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79f6de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = persistedModel.transform(poorHealthSample)\n",
    "predictions.select(\"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8748f1fd",
   "metadata": {},
   "source": [
    "#### 5.2 Predicting on person with good health and habits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f19e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "goodHealthPerson = [{ \"HeartDisease\" : -1, \"State\" : 22 , \"BMI\" : 20, \"HighBP\" : 0, \"HighChol\" : 0, \"CholCheck\" :0, \\\n",
    "             \"FruitConsume\" : 1, \"VegetableConsume\" : 1 , \"Smoker\" : 0 , \"HeavyDrinker\" : 0, \\\n",
    "                    \"Diabetes\" : 0 , \"Stroke\" : 1 , \"Healthcare\": 1 , \"NoDoctorDueToCost\" : 0, \\\n",
    "                    \"PhysicalActivity\" : 1 , \"GeneralHealth\": 4 ,\"PhysicalHealth\" : 3 , \"MentalHealth\" : 1 ,\\\n",
    "                    \"DifficultyWalking\" : 0 , \"Gender\" : 0 ,\"Age\" : 10 , \"Education\" : 2 , \"Income\": 1 }]\n",
    "             \n",
    "dfGoodHealth = spark.createDataFrame(goodHealthPerson)\n",
    "assembler = VectorAssembler(inputCols=numericCols, outputCol=\"features\")\n",
    "goodHealthSample= assembler.transform(dfGoodHealth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68fb693",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = persistedModel.transform(goodHealthSample)\n",
    "predictions.select(\"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a79f40",
   "metadata": {},
   "source": [
    "#### 5.3 Enter your stats to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1517a60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BMI = 21\n",
    "HighBP = 0\n",
    "HighChol = 0\n",
    "CholCheck = 0\n",
    "c = 0\n",
    "VegetableConsume = 1\n",
    "Smoker = 0\n",
    "HeavyDrinker = 0\n",
    "Diabetes = 0 \n",
    "Stroke = 0\n",
    "Healthcare = 1\n",
    "NoDoctorDueToCost = 0\n",
    "PhysicalActivity = 0\n",
    "GeneralHealth = 4\n",
    "PhysicalHealth = 2\n",
    "MentalHealth = 0\n",
    "DifficultyWalking = 0\n",
    "Gender = 0\n",
    "Age = 3\n",
    "Education = 4\n",
    "Education = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99108168",
   "metadata": {},
   "outputs": [],
   "source": [
    "yourData = [{ \"HeartDisease\" : -1, \"State\" : 22 , \"BMI\" : BMI, \"HighBP\" : HighBP, \"HighChol\" : HighChol, \"CholCheck\" :CholCheck, \\\n",
    "             \"FruitConsume\" : CholCheck, \"VegetableConsume\" : VegetableConsume , \"Smoker\" : Smoker , \"HeavyDrinker\" : HeavyDrinker, \\\n",
    "                    \"Diabetes\" : Diabetes , \"Stroke\" : Stroke , \"Healthcare\": Healthcare , \"NoDoctorDueToCost\" : NoDoctorDueToCost, \\\n",
    "                    \"PhysicalActivity\" : PhysicalActivity , \"GeneralHealth\": GeneralHealth ,\"PhysicalHealth\" : PhysicalHealth , \"MentalHealth\" : MentalHealth ,\\\n",
    "                    \"DifficultyWalking\" : DifficultyWalking , \"Gender\" : Gender,\"Age\" : Age , \"Education\" : Education , \"Income\": Education }]\n",
    "             \n",
    "dfYourData = spark.createDataFrame(yourData)\n",
    "assembler = VectorAssembler(inputCols=numericCols, outputCol=\"features\")\n",
    "yourSample= assembler.transform(dfYourData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6789ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = persistedModel.transform(yourSample)\n",
    "print( \"Prediction based on your data are as follows: \")\n",
    "predictions.select(\"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becbeadd",
   "metadata": {},
   "source": [
    "# ---- END ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a315c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
